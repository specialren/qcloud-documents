## 操作场景

集群是 TDMQ RocketMQ 版中的一个资源维度，不同集群的命名空间、Topic、Group 等完全隔离。每个集群会有集群的资源限制例如 Topic 总数、消息保留时长等。常见的使用方式如：开发测试环境使用一个专门集群，生产环境使用一个专门的集群。

集群分为虚拟集群和专享集群：

- **专享集群**： 独占物理资源，数据安全，使用几乎无限制。
- **虚拟集群**： 使用虚拟化的计算和存储资源，根据用量自动分配，使用有一定限制。

**TDMQ 资源层次关系**
![](https://main.qcloudimg.com/raw/a98447d277622f79a33a9e5376d4ea95.png)

## 操作步骤

### 创建集群

1. 登录 [TDMQ 控制台](https://console.cloud.tencent.com/tdmq)，进入**集群管理**页面。
2. 在**集群管理**页面，选择地域后，单击**新建集群**进入新建集群对话框。
  - 集群类型：支持**专享集群**和**虚拟集群**两种类型。
    - **专享集群**： 独占物理资源，数据安全，使用几乎无限制。
    - **虚拟集群**： 使用虚拟化的计算和存储资源，根据用量自动分配，使用有一定限制。
  - 计费模式：专享集群使用**包年包月**计费模式，虚拟集群使用**按量计费**模式。
  - 地域：选择与您的业务最靠近的地域，处于不同地域的云产品内网不通，购买后不能更换，请您谨慎选择。例如，广州地域的云服务器无法通过内网访问上海地域的集群。若需要跨地域内网通信，请参见 [对等连接](https://cloud.tencent.com/document/product/553/18836)。
  - 集群规格：根据您的业务需求选择合适的集群规格。
  - 网络配置：默认不开通公网访问，如需开通请 [提交申请](https://cloud.tencent.com/login?s_url=https%3A%2F%2Fconsole.cloud.tencent.com%2Fworkorder%2Fcategory%3Flevel1_id%3D876%26level2_id%3D1772%26source%3D0%26data_title%3D%25E6%25B6%2588%25E6%2581%25AF%25E9%2598%259F%25E5%2588%2597%2520TDMQ%26step%3D1)。
  - 集群名称：填写集群名称， 3-64个字符，只能包含数字、字母、“-”和“\_”。
  - 标签：标签用于从不同维度对资源分类管理。使用方法请参见 [使用标签管理资源](https://cloud.tencent.com/document/product/1493/63447)。
3. 单击**立即购买**，完成集群创建。

后续步骤：以下 TDMQ CMQ 版简称为新版 CMQ，当前的消息队列 CMQ 简称为原 CMQ。

### 基础数据迁移相关

#### 问题场景：
单击 CMQ 界面上的基础数据后发现新 CMQ 订阅的数量和原 CMQ 的订阅数量不符。

#### 问题说明：
其实相差的这部分是曾经在原 CMQ 已经被人为删除了的队列，而原 CMQ 在查看订阅时不会严格过滤已经删除的队列，实际上这些“多余”的订阅者早已失去了相关的消费逻辑。但是在新 CMQ 的订阅逻辑里，只要队列删除了，相关的主题订阅关系也一并会被删除，因此不会展示这部分订阅者。

#### 解决方案：
如果您希望验证这个问题，可以将原 CMQ 里“没有迁移过来”的订阅队列名输入到原 CMQ 队列列表的搜索框中搜索一下，没有结果则证明该订阅关系在新 CMQ 的缺失属于上述情况。


### 消费时延明显增长

#### 问题场景：
切换到新版 CMQ 后发现消息拉取的时间明显变长（可能达到10s - 20s）。此类问题属于已知问题，常出现在消息量不大，且消费者不多的场景。原因在于新版的分布式架构中，拉取消息轮询各节点的随机性在消息数量少的时候体现得更为明显，使得底层会有一些反复轮询的现象，导致时延增加。

#### 解决方案：

1. 队列属性中的“消息接收长轮询等待时间”改小，推荐调整为3秒。
2. 我们准备了专门针对这种场景特化的集群，降低轮询耗时，**用户将客户端的接入点切换为这些集群即可明显降低消息拉取的时耗**（无需重建队列）。
3. 以下表格中，同地域的多个接入点均可使用，底层均对应相同的场景特化集群。

目前已支持特化集群的有以下地域：

|地域|接入点|
|-|-|
|广州| **公网**<li>https://tcmq-gz.public.tencenttdmq.com</li><li>https://cmq-gz.public.tencenttdmq.com:9443</li> **内网**<li>http://tcmq-gz.mqadapter.tencentyun.com:8090</li><li>http://gz.mqadapter.cmq.tencentyun.com:8080
</li>|
|上海| **公网**<li>https://tcmq-sh.public.tencenttdmq.com</li><li>https://cmq-sh.public.tencenttdmq.com:9443</li> **内网**<li>http://tcmq-sh.mqadapter.tencentyun.com:8090</li><li>http://sh.mqadapter.cmq.tencentyun.com:8080
</li>|
|北京| **公网**<li>https://tcmq-bj.public.tencenttdmq.com</li><li>https://cmq-bj.public.tencenttdmq.com:8443</li> **内网**<li>http://tcmq-bj.mqadapter.tencentyun.com:8090</li><li>http://bj.mqadapter.cmq.tencentyun.com:8080
</li>|
|中国香港| **公网**<li>https://tcmq-hk.public.tencenttdmq.com</li><li>https://cmq-hk.public.tencenttdmq.com:8443</li> **内网**<li>http://tcmq-hk.mqadapter.tencentyun.com:8090</li><li>http://hk.mqadapter.cmq.tencentyun.com:8080
</li>|
|新加坡| **公网**<li>https://tcmq-sg.public.tencenttdmq.com</li> **内网**<li>http://tcmq-sg.mqadapter.tencentyun.com:8090
</li>|


### 管控类 API(新增/查看队列等)不兼容
#### 问题场景：
切换过来后 SDK 中 createQueue 等管控类的方法报错。

#### 问题说明：
新版 CMQ 对于原来 SDK 中管控流的操作是不兼容的（创建队列、查看队列列表等接口）。 腾讯云对控制台整体的云 API 做了一次升级（V2 到 V3），不再提供原来 V2 协议的接口，因此新版 CMQ 管控流相关的接口需要遵照最新云 API 的协议进行改造，改造后性能和开发友好性更高。具体可以参见 [API 文档](https://cloud.tencent.com/document/product/1496/62819)。

新版 CMQ 管控流和数据流做了分离，原则上是这两类的调用场景和性质不同，区分可以更好提供服务，因此调用的域名也不相同：
数据流的调用地址请在控制台获取；管控流的调用地址请参考具体的云 API 的文档，例如 [创建 CMQ 队列](https://cloud.tencent.com/document/api/1179/55917) API 文档。

### 原先使用的 TCP 协议的 SDK，新版 CMQ 出错

新版 CMQ 默认仅支持 HTTP 协议，如果原先有使用 TCP 协议的 SDK 且希望无缝迁移，我们提供了 TCP  SDK 的专属接入点，如下所示：

|地域|接入点|
|-|-|
|广州| <li>http://gz.mqadapter.cmq.tencentyun.com (内网)</li><li>https://cmq-gz.public.tencenttdmq.com (公网)</li>|
|上海| <li>http://sh.mqadapter.cmq.tencentyun.com (内网)</li><li>https://cmq-sh.public.tencenttdmq.com (公网)</li>|
|北京| <li>http://bj.mqadapter.cmq.tencentyun.com (内网)</li><li>https://cmq.bj.public.tencenttdmq.com (公网)</li>|

TCP 的客户端配置成 TCP 专用的接入点即可解决。


### 存量系统复杂迁移困难

如果您发现您正在使用 CMQ 的系统较为复杂或由于各类原因难以维护，导致迁移困难，请及时通过 [工单](https://console.cloud.tencent.com/workorder/category?level1_id=876&level2_id=947&source=14&data_title=%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%20CMQ&step=1) 与我们联系，我们将提供后台切换的方案协助您进行迁移。


<dx-alert infotype="notice" title="">
<li>后台切换会有一定的延时增长</li>
<li>后台切换完成后，新增或者编辑的操作请勿在原版 CMQ 进行</li>
</dx-alert>





1. 在集群中创建 [命名空间](https://cloud.tencent.com/document/product/1493/61595)，并获取接入地址，得到服务端的连接信息。
2. 在集群中创建 [角色](https://cloud.tencent.com/document/product/1493/61599) 并授予该命名空间的生产消费权限。
3. 在命名空间中创建 [Topic](https://cloud.tencent.com/document/product/1493/61597)。
4. 按照 [SDK 文档](https://cloud.tencent.com/document/product/1493/61652) 的提示编写 Demo，配置上连接信息，进行消息的生产和消费。

### 查看集群详情

在**集群管理**列表页，单击操作列的**查看详情**，进入集群详情页面。在详情页中，您可以查询到：

- 集群概况（Topic 数量、近24小时消息生产数量、当前消息堆积数量）
- 集群的基础信息（集群名称/ID、地域、创建时间、说明）
- 实例配置：
  <table>
  <thead>
  <tr>
  <th>实例配置</th>
  <th>配置说明</th>
  </tr>
  </thead>
  <tbody><tr>
  <td>单 Topic 分区 TPS 上限</td>
  <td>生产消费分开限制，即生产 TPS 到达上限不影响消费 TPS。</td>
  </tr>
  <tr>
  <td>命名空间数量上限</td>
  <td>命名空间可创建的最大数量。</td>
  </tr>
  <tr>
  <td>Topic 数量上限</td>
  <td>Topic 可创建的最大数量。</td>
  </tr>
  <tr>
  <td>Group 数量上限</td>
  <td>Group 可创建的最大数量。</td>
  </tr>
  <tr>
  <td>消息保留时间</td>
  <td>消息保留可配置的最长时间。</td>
  </tr>
  <tr>
  <td>消息最大延时</td>
  <td>消息延迟消费的最长时间。</td>
  </tr>
  </tbody></table>
  

![](https://qcloudimg.tencent-cloud.cn/raw/7f982eb4da3986f01adb07f1e9c7f0bb.png)

### 获取接入地址

2022年8月8日部分地区上线新的虚拟集群 RocketMQ 服务，新版集群在命名空间列表获取相应的接入地址，之前创建的集群或未升级的集群仍在集群操作列的**接入地址**处获取。

- 新版集群：
![](https://qcloudimg.tencent-cloud.cn/raw/91e590fce98e38121b3efa7243864128.png)
- 旧版集群：
![](https://main.qcloudimg.com/raw/4fed5e6a2497995613b645f6a9a7e206.png)

### 升级集群规格

如当前的集群规格不满足您的业务需求，您可以在控制台上提升您的节点数量。

> ?当前仅支持提升节点数量，暂不支持修改节点规格和存储规格（该功能会下个版本中支持）。

1. 在**集群管理**列表页，单击操作列的**升配**。
2. 修改节点数量后，单击**确认调整**。
  ![](https://qcloudimg.tencent-cloud.cn/raw/d5fb531836f8c35b845dac6073d21e67.png)
  

### 编辑集群

1. 在**集群管理**列表页，单击操作列的**编辑**。
2. 在弹窗中填写集群名称和集群说明，单击提交。

### 删除集群

如果想删掉创建的集群，可以通过以下步骤操作：

1. 在**集群管理**列表页，单击操作列的**删除**。
2. 在删除的确认弹框中，单击**删除**，即可删除集群。

> !删除后，该集群下的所有配置都会被清空，且无法恢复，请谨慎操作。

